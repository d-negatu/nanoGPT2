# nanoGPT2 ğŸ§ 

[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A minimal-yet-practical GPT-2 style transformer implemented from scratch for learning, research, and experimentation. Clean, readable code with comments and testsâ€”focused on understanding how modern LLMs work end-to-end.

## âœ¨ Goals
- Pedagogical clarity: each component is small and well-documented
- Practical training: train on small datasets on a single GPU/CPU
- Reproducible results: seeds, configs, and deterministic dataloaders
- Extensible: easy to add new blocks, attention variants, and schedulers

## ğŸ§© Features
- Tokenizer (BPE or byte-level)
- Transformer blocks (MHSA, MLP, LayerNorm, residuals)
- Causal attention with attention masking
- Weight tying between embeddings and output head
- Configurable model sizes (nano, small, base)
- Trainer with gradient clipping, mixed precision (optional)
- Cosine LR scheduler with warmup
- Checkpointing and resume support

## ğŸ“¦ Installation
```bash
git clone https://github.com/d-negatu/nanoGPT2.git
cd nanoGPT2
pip install -e .
```

## ğŸš€ Quickstart
Train a tiny model on tiny Shakespeare:
```bash
python scripts/prepare_shakespeare.py
python train.py --config configs/nano.yaml
```

Sample text:
```bash
python sample.py --checkpoint runs/nano/latest.pt --max-new-tokens 200 --temperature 0.9
```

## ğŸ› ï¸ Project Structure
```
nanoGPT2/
â”œâ”€â”€ gpt2/
â”‚   â”œâ”€â”€ model.py          # Transformer model
â”‚   â”œâ”€â”€ blocks.py         # Attention + MLP blocks
â”‚   â”œâ”€â”€ tokenizer.py      # Byte-level or BPE tokenizer
â”‚   â””â”€â”€ utils.py          # Positional encodings, masks, etc.
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_shakespeare.py
â”‚   â””â”€â”€ prepare_openwebtext.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ nano.yaml         # ~1-5M params
â”‚   â”œâ”€â”€ small.yaml        # ~10-30M params
â”‚   â””â”€â”€ base.yaml         # ~100M params
â”œâ”€â”€ train.py              # Training loop
â”œâ”€â”€ sample.py             # Sampling/generation
â”œâ”€â”€ tests/                # Unit tests
â””â”€â”€ README.md
```

## âš™ï¸ Example Config (nano.yaml)
```yaml
model:
  vocab_size: 50304
  n_layer: 6
  n_head: 6
  n_embd: 384
  dropout: 0.1
train:
  seq_len: 256
  batch_size: 64
  max_steps: 10000
  lr: 3e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  warmup_steps: 200
  grad_clip: 1.0
  amp: false
```

## ğŸ“ˆ Tips for Good Results
- Start with seq_len=128 to iterate quickly, then increase
- Use cosine decay with warmup for stable convergence
- Enable AMP on GPUs for 1.5-2x speedup
- Keep attention heads divisible by embedding dim

## ğŸ§ª Testing
```bash
pytest -q
```

## ğŸ”­ Roadmap
- [ ] Rotary embeddings (RoPE)
- [ ] FlashAttention (fallback implementation)
- [ ] LoRA fine-tuning
- [ ] Export to ONNX
- [ ] Web demo with Gradio

## ğŸ“š References
- Attention Is All You Need (Vaswani et al., 2017)
- Language Models are Unsupervised Multitask Learners (Radford et al., 2019)
- GPT-2 open-source reimplementations

## ğŸ‘¤ Author
Dagmawi Negatu â€” Western Carolina University
- GitHub: https://github.com/d-negatu
- LinkedIn: https://www.linkedin.com/in/danegatu

If this repo helps you learn, please â­ it to support the project!
