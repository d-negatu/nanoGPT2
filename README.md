# nanoGPT2 ğŸ§ 

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/d-negatu/nanoGPT2/blob/main/notebooks/Quick_Start_nanoGPT2.ipynb)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A minimal-yet-practical GPT-2 style transformer implemented from scratch for learning, research, and experimentation. Clean, readable code with comments and testsâ€”focused on understanding how modern LLMs work end-to-end.

## âœ¨ Goals

- Pedagogical clarity: each component is small and well-documented
- Practical training: train on small datasets on a single GPU/CPU
- Reproducible results: seeds, configs, and deterministic dataloaders
- Extensible: easy to add new blocks, attention variants, and schedulers

## ğŸ§© Features

- Tokenizer (BPE or byte-level)
- Transformer blocks (MHSA, MLP, LayerNorm, residuals)
- Causal attention with attention masking
- Weight tying between embeddings and output head
- Configurable model sizes (nano, small, base)
- Trainer with gradient clipping, mixed precision (optional)
- Cosine LR scheduler with warmup
- Checkpointing and resume support

## ğŸ“¦ Installation

```bash
git clone https://github.com/d-negatu/nanoGPT2.git
cd nanoGPT2
pip install -e .
```

## ğŸ¬ Demo

![nanoGPT2 Demo](docs/demo.svg)

Quick demonstration of nanoGPT2 capabilities:
- Minimal GPT model with 4 layers
- Train on custom text datasets
- Generate text with a simple API
- Run on GPU or CPU
- Interactive Colab notebook available

### Try it Now

Get started immediately with our interactive Colab notebook:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/d-negatu/nanoGPT2/blob/main/notebooks/Quick_Start_nanoGPT2.ipynb)

## ğŸš€ Quickstart

Train a tiny model on tiny Shakespeare:

```bash
python scripts/prepare_shakespeare.py
python train.py --config configs/nano.yaml
```

Sample text:

```bash
python sample.py --checkpoint runs/nano/latest.pt --max-new-tokens 200 --temperature 0.9
```

## ğŸ› ï¸ Project Structure

```
nanoGPT2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py          # Transformer model
â”‚   â”œâ”€â”€ blocks.py         # Attention + MLP blocks
â”‚   â”œâ”€â”€ tokenizer.py      # Byte-level or BPE tokenizer
â”‚   â”œâ”€â”€ trainer.py        # Training loop
â”‚   â””â”€â”€ utils.py          # Positional encodings, masks, etc.
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Quick_Start_nanoGPT2.ipynb  # Interactive Colab notebook
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_shakespeare.py
â”‚   â””â”€â”€ demo.py           # Generate demo outputs
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ demo.svg          # Demo visualization
â”‚   â”œâ”€â”€ architecture.md    # Model architecture details
â”‚   â”œâ”€â”€ training.md        # Training guide
â”‚   â””â”€â”€ api.md             # API reference
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ nano.yaml         # Smallest config
â”‚   â”œâ”€â”€ small.yaml        # Small config
â”‚   â””â”€â”€ base.yaml         # Base config
â”œâ”€â”€ train.py              # Main training script
â”œâ”€â”€ sample.py             # Sampling script
â”œâ”€â”€ CHANGELOG.md          # Release notes and changelog
â””â”€â”€ README.md             # This file
```

## ğŸ“š Documentation

For more detailed information, check out our documentation:

- **[Architecture Guide](docs/architecture.md)** - Deep dive into the model architecture
- **[Training Guide](docs/training.md)** - Tips and tricks for training your own models
- **[API Reference](docs/api.md)** - Complete API documentation
- **[Changelog](CHANGELOG.md)** - Release notes and feature history
- **[Interactive Notebook](notebooks/Quick_Start_nanoGPT2.ipynb)** - Try it on Colab

## ğŸ’¡ Usage Examples

### Basic Training

```python
from src.model import GPT
from src.trainer import Trainer

config = {'vocab_size': 256, 'block_size': 128, 'n_layer': 4}
model = GPT(config)
trainer = Trainer(model, config)
trainer.train()
```

### Text Generation

```python
import torch

context = torch.zeros((1, 1), dtype=torch.long)
samples = model.generate(context, max_new_tokens=100)
print(samples)
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to open issues and submit pull requests.

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

Inspired by [nanoGPT](https://github.com/karpathy/nanoGPT) and educational resources on transformers.
